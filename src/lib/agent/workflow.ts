import OpenAI from 'openai';
import { 
  InterviewState, 
  NodeId, 
  AgentResponse, 
  QuestionMessage, 
  ModelPlan,
  StreamingMessage,
  CompletedDocument,
  Persona,
  Interview,
  EvaluationResult,
  ExternalEnvironmentAnalysis,
  ProfitabilityAssessment,
  FeasibilityAssessment,
  LegalAssessment
} from '@/lib/types/agent';

export class AgentWorkflow {
  private openai: OpenAI;

  constructor() {
    this.openai = new OpenAI({
      apiKey: process.env.OPENAI_API_KEY!,
    });
  }

  async executeNode(
    node: NodeId, 
    state: InterviewState, 
    userResponse?: string
  ): Promise<{ response: AgentResponse; nextState: InterviewState; nextNode?: NodeId }> {
    
    console.log(`=== Executing Node: ${node} ===`);
    console.log('User response:', userResponse);
    console.log('Current state:', {
      initial_problem: state.initial_problem,
      clarification_log_length: state.clarification_interview_log.length,
      user_request_length: state.user_request.length,
      current_question_index: state.current_question_index
    });

    switch (node) {
      case "clarification_interview":
        return this.handleClarificationInterview(state, userResponse);
      
      case "detailed_questions":
        return this.handleDetailedQuestions(state, userResponse);
      
      case "summarize_request":
        return this.handleSummarizeRequest(state);
      
      case "generate_personas":
        return this.handleGeneratePersonas(state, userResponse);
      
      case "conduct_interviews":
        return this.handleConductInterviews(state, userResponse);
      
      case "evaluate_information":
        return this.handleEvaluateInformation(state);
      
      case "ask_followups":
        return this.handleAskFollowups(state, userResponse);
      
      case "generate_professional_requirements":
        return this.handleGenerateProfessionalRequirements(state);
      
      case "analyze_environment":
        return this.handleAnalyzeEnvironment(state);
      
      case "assess_profitability":
        return this.handleAssessProfitability(state);
      
      case "assess_feasibility":
        return this.handleAssessFeasibility(state);
      
      case "assess_legal":
        return this.handleAssessLegal(state);
      
      case "assessment_gate":
        return this.handleAssessmentGate(state);
      
      case "improve_requirements":
        return this.handleImproveRequirements(state);
      
      case "generate_pitch":
        return this.handleGeneratePitch(state);
      
      default:
        throw new Error(`Unknown node: ${node}`);
    }
  }

  private async handleClarificationInterview(
    state: InterviewState, 
    userResponse?: string
  ): Promise<{
    response: AgentResponse; 
    nextState: InterviewState; 
    nextNode?: NodeId;
  }> {
    console.log('=== handleClarificationInterview ===');
    console.log('Has user response:', !!userResponse);
    console.log('Current question index:', state.current_question_index);
    console.log('Service overview:', state.clarification_answers['service_overview']);
    console.log('Current state answers:', {
      service_overview: state.clarification_answers['service_overview'],
      problem: state.clarification_answers['problem'],
      persona: state.clarification_answers['persona'],
      solution: state.clarification_answers['solution']
    });

    // Define the questions (after service overview)
    const questions = [
      {
        id: 'problem',
        prompt: 'è§£æ±ºã—ãŸã„èª²é¡Œã¯ä½•ã§ã™ã‹ï¼Ÿ',
        placeholder: 'ä¾‹: æ­Œã‚’æ­Œã£ã¦ã„ã‚‹ã¨ãã€ä¸€äººã ã¨å¯‚ã—ã„',
        key: 'problem'
      },
      {
        id: 'persona',
        prompt: 'ã“ã®èª²é¡Œã‚’æŒã¤ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼ˆãƒšãƒ«ã‚½ãƒŠï¼‰ã¯èª°ã§ã™ã‹ï¼Ÿ',
        placeholder: 'ä¾‹: ã‚«ãƒ©ã‚ªã‚±ãŒå¥½ããª20ä»£ã®ç¤¾ä¼šäºº',
        key: 'persona'
      },
      {
        id: 'solution',
        prompt: 'ã©ã®ã‚ˆã†ãªè§£æ±ºç­–ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã‹ï¼Ÿ',
        placeholder: 'ä¾‹: AIãŒè‡ªå‹•ã§ãƒãƒ¢ã£ã¦ãã‚Œã‚‹ã‚¢ãƒ—ãƒª',
        key: 'solution'
      }
    ];

    // Process user response
    if (userResponse && userResponse.trim()) {
      // First response is the service overview
      if (!state.clarification_answers['service_overview']) {
        console.log('Saving service overview and showing first question');
        
        const updatedAnswers = {
          ...state.clarification_answers,
          service_overview: userResponse
        };
        
        const newState: InterviewState = {
          ...state,
          clarification_answers: updatedAnswers,
          current_question_index: 0  // Start with first question
        };
        
        // Show the first question (problem)
        const response: QuestionMessage = {
          type: "question",
          content: questions[0].prompt,
          placeholder: questions[0].placeholder,
          node: "clarification_interview",
          key: questions[0].key,
          currentQuestion: 1,
          totalQuestions: 3
        };
        
        return {
          response,
          nextState: newState
        };
      }
      
      // Process answers to the 3 questions
      console.log('Processing answer for question index:', state.current_question_index);
      
      const currentQuestionKey = questions[state.current_question_index]?.key;
      
      if (currentQuestionKey) {
        // Save the answer
        const updatedAnswers = {
          ...state.clarification_answers,
          [currentQuestionKey]: userResponse
        };

        const nextIndex = state.current_question_index + 1;
        
        // Check if all 3 questions are completed
        if (nextIndex >= 3) {
          console.log('All 3 questions completed, moving to summarize_request');
          
          // Format the collected information
          const clarificationLog = `## åé›†ã—ãŸæƒ…å ±\n\n` +
            `### ã‚µãƒ¼ãƒ“ã‚¹æ¦‚è¦\n${updatedAnswers['service_overview']}\n\n` +
            `### æƒ³å®šèª²é¡Œ\n${updatedAnswers['problem']}\n\n` +
            `### ãƒšãƒ«ã‚½ãƒŠ\n${updatedAnswers['persona']}\n\n` +
            `### æƒ³å®šè§£æ±ºç­–\n${updatedAnswers['solution']}`;
          
          const finalState: InterviewState = {
            ...state,
            initial_problem: updatedAnswers['problem'] || state.initial_problem,
            initial_persona: updatedAnswers['persona'] || state.initial_persona,
            initial_solution: updatedAnswers['solution'] || state.initial_solution,
            clarification_answers: updatedAnswers,
            clarification_interview_log: clarificationLog,
            current_question_index: nextIndex
          };

          const response: ModelPlan = {
            type: "plan",
            nextNode: "detailed_questions",
            statePatch: { 
              initial_problem: finalState.initial_problem,
              initial_persona: finalState.initial_persona,
              initial_solution: finalState.initial_solution,
              clarification_answers: updatedAnswers,
              clarification_interview_log: clarificationLog,
              current_question_index: nextIndex
            }
          };

          return {
            response,
            nextState: finalState,
            nextNode: "detailed_questions"
          };
        }

        // Move to next question
        const nextQuestion = questions[nextIndex];
        const newState: InterviewState = {
          ...state,
          clarification_answers: updatedAnswers,
          current_question_index: nextIndex
        };

        const response: QuestionMessage = {
          type: "question",
          content: nextQuestion.prompt,
          placeholder: nextQuestion.placeholder,
          node: "clarification_interview",
          key: nextQuestion.key,
          currentQuestion: nextIndex + 1,
          totalQuestions: 3
        };

        console.log('Moving to question:', nextQuestion.id);

        return {
          response,
          nextState: newState
        };
      }
    }

    // Show current question if no response yet (only after service overview is collected)
    if (!userResponse || !userResponse.trim()) {
      // If we don't have service overview yet, wait for it (don't show questions yet)
      if (!state.clarification_answers['service_overview']) {
        console.log('Waiting for service overview - not showing questions yet');
        // Return a plan to stay in the same node and wait for user input
        const response: QuestionMessage = {
          type: "question",
          content: "ã‚µãƒ¼ãƒ“ã‚¹ã®æ¦‚è¦ã‚’æ•™ãˆã¦ãã ã•ã„",
          placeholder: "ä¾‹: æ­Œã‚’æ­Œã†ã¨AIãŒè‡ªå‹•ã§ãƒãƒ¢ã£ã¦ãã‚Œã‚‹ã‚¢ãƒ—ãƒª",
          node: "clarification_interview",
          key: "service_overview",
          currentQuestion: 0,
          totalQuestions: 4
        };
        
        return {
          response,
          nextState: state
        };
      }
      
      // Show current question if service overview is already collected
      const currentIndex = state.current_question_index;
      if (currentIndex < 3) {
        const currentQuestion = questions[currentIndex];
        
        const response: QuestionMessage = {
          type: "question",
          content: currentQuestion.prompt,
          placeholder: currentQuestion.placeholder,
          node: "clarification_interview",
          key: currentQuestion.key,
          currentQuestion: currentIndex + 1,
          totalQuestions: 3
        };

        console.log('Showing question:', currentQuestion.id, 'at index:', currentIndex);

        return {
          response,
          nextState: state
        };
      }
    }

    // All questions completed
    console.log('All questions already completed, moving to next step');
    const response: ModelPlan = {
      type: "plan",
      nextNode: "summarize_request"
    };

    return {
      response,
      nextState: state,
      nextNode: "summarize_request"
    };
  }

  private async handleDetailedQuestions(
    state: InterviewState, 
    userResponse?: string
  ): Promise<{
    response: AgentResponse; 
    nextState: InterviewState; 
    nextNode?: NodeId;
  }> {
    console.log('=== handleDetailedQuestions ===');
    console.log('Has user response:', !!userResponse);
    console.log('Current detailed question index:', state.current_detailed_question_index);
    console.log('Total detailed questions:', state.detailed_questions.length);

    // Generate detailed questions if not generated yet
    if (state.detailed_questions.length === 0) {
      console.log('Generating detailed questions based on collected inputs');
      
      const problem = state.clarification_answers['problem'] || state.initial_problem;
      const persona = state.clarification_answers['persona'] || state.initial_persona;
      const solution = state.clarification_answers['solution'] || state.initial_solution;
      
      const detailedQuestions = await this.generateDetailedQuestions(problem, persona, solution);
      
      const newState: InterviewState = {
        ...state,
        detailed_questions: detailedQuestions,
        current_detailed_question_index: 0
      };
      
      // Show first detailed question
      const firstQuestion = detailedQuestions[0];
      const response: QuestionMessage = {
        type: "question",
        content: firstQuestion,
        choices: [
          { label: "ã¯ã„", value: "ã¯ã„" },
          { label: "ã„ã„ãˆ", value: "ã„ã„ãˆ" },
          { label: "ã‚ã‹ã‚‰ãªã„", value: "ã‚ã‹ã‚‰ãªã„" }
        ],
        node: "detailed_questions",
        key: "detailed_0",
        currentQuestion: 1,
        totalQuestions: detailedQuestions.length
      };
      
      return {
        response,
        nextState: newState
      };
    }
    
    // Process user response to detailed question
    if (userResponse && userResponse.trim()) {
      console.log('Processing detailed question answer:', state.current_detailed_question_index);
      
      const updatedAnswers = {
        ...state.detailed_answers,
        [`detailed_${state.current_detailed_question_index}`]: userResponse
      };
      
      const nextIndex = state.current_detailed_question_index + 1;
      
      // Check if all detailed questions are completed
      if (nextIndex >= state.detailed_questions.length) {
        console.log('All detailed questions completed, moving to summarize_request');
        
        // Format detailed question log
        const detailedLog = this.formatDetailedAnswersAsLog(state.detailed_questions, updatedAnswers);
        
        const finalState: InterviewState = {
          ...state,
          detailed_answers: updatedAnswers,
          clarification_interview_log: state.clarification_interview_log + '\n\n' + detailedLog,
          current_detailed_question_index: nextIndex
        };
        
        const response: ModelPlan = {
          type: "plan",
          nextNode: "summarize_request",
          statePatch: {
            detailed_answers: updatedAnswers,
            clarification_interview_log: finalState.clarification_interview_log,
            current_detailed_question_index: nextIndex
          }
        };
        
        return {
          response,
          nextState: finalState,
          nextNode: "summarize_request"
        };
      }
      
      // Move to next detailed question
      const nextQuestion = state.detailed_questions[nextIndex];
      const newState: InterviewState = {
        ...state,
        detailed_answers: updatedAnswers,
        current_detailed_question_index: nextIndex
      };
      
      const response: QuestionMessage = {
        type: "question",
        content: nextQuestion,
        choices: [
          { label: "ã¯ã„", value: "ã¯ã„" },
          { label: "ã„ã„ãˆ", value: "ã„ã„ãˆ" },
          { label: "ã‚ã‹ã‚‰ãªã„", value: "ã‚ã‹ã‚‰ãªã„" }
        ],
        node: "detailed_questions",
        key: `detailed_${nextIndex}`,
        currentQuestion: nextIndex + 1,
        totalQuestions: state.detailed_questions.length
      };
      
      return {
        response,
        nextState: newState
      };
    }
    
    // Show current detailed question if no response
    const currentIndex = state.current_detailed_question_index;
    if (currentIndex < state.detailed_questions.length) {
      const currentQuestion = state.detailed_questions[currentIndex];
      
      const response: QuestionMessage = {
        type: "question",
        content: currentQuestion,
        choices: [
          { label: "ã¯ã„", value: "ã¯ã„" },
          { label: "ã„ã„ãˆ", value: "ã„ã„ãˆ" },
          { label: "ã‚ã‹ã‚‰ãªã„", value: "ã‚ã‹ã‚‰ãªã„" }
        ],
        node: "detailed_questions",
        key: `detailed_${currentIndex}`,
        currentQuestion: currentIndex + 1,
        totalQuestions: state.detailed_questions.length
      };
      
      return {
        response,
        nextState: state
      };
    }
    
    // All questions completed, move to next step
    const response: ModelPlan = {
      type: "plan",
      nextNode: "summarize_request"
    };
    
    return {
      response,
      nextState: state,
      nextNode: "summarize_request"
    };
  }

  private async handleSummarizeRequest(state: InterviewState): Promise<{
    response: AgentResponse;
    nextState: InterviewState;
    nextNode?: NodeId;
  }> {
    console.log('=== handleSummarizeRequest ===');
    console.log('Input data:', {
      initial_problem: state.initial_problem,
      initial_persona: state.initial_persona,
      initial_solution: state.initial_solution,
      clarification_answers: state.clarification_answers
    });

    // Generate summary from the 3 collected inputs
    const summary = await this.generateRequestSummary(
      state.initial_problem,
      state.initial_persona,
      state.initial_solution,
      state.clarification_interview_log
    );

    console.log('Generated summary:', summary);

    const newState: InterviewState = {
      ...state,
      user_request: summary
    };

    // Display the summary to the user as a completed document
    const response: CompletedDocument = {
      type: "completed",
      documentType: "summary",
      title: "ã‚µãƒ¼ãƒ“ã‚¹æ¦‚è¦",
      content: summary,
      node: "summarize_request"
    };

    console.log('Returning completed summary document');

    return {
      response,
      nextState: newState,
      nextNode: "generate_personas"
    };
  }

  private async handleGeneratePersonas(
    state: InterviewState, 
    userResponse?: string
  ): Promise<{
    response: AgentResponse;
    nextState: InterviewState; 
    nextNode?: NodeId;
  }> {
    console.log('=== handleGeneratePersonas ===');
    console.log('User response:', userResponse);
    
    // If user confirmed personas, move to interviews
    if (userResponse && userResponse.includes("ã¯ã„ã€ã“ã®è¨­å®šã§é€²ã‚ã¦ãã ã•ã„")) {
      console.log('User confirmed personas, moving to conduct_interviews');
      
      const response: ModelPlan = {
        type: "plan",
        nextNode: "conduct_interviews"
      };

      return {
        response,
        nextState: state,
        nextNode: "conduct_interviews"
      };
    }
    
    // Generate personas if not already generated
    if (!state.personas || state.personas.length === 0) {
      try {
        const personas = await this.generatePersonas(state.user_request);
        console.log('Generated personas:', personas.length);
        
        if (personas.length === 0) {
          console.error('No personas generated, returning error');
          // Return error state if no personas were generated
          const response: QuestionMessage = {
            type: "question",
            content: "ãƒšãƒ«ã‚½ãƒŠã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ã„ãŸã ãã‹ã€æ‰‹å‹•ã§ãƒšãƒ«ã‚½ãƒŠã‚’è¨­å®šã—ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ",
            choices: [
              { label: "å†è©¦è¡Œã™ã‚‹", value: "å†è©¦è¡Œã™ã‚‹" },
              { label: "æ‰‹å‹•ã§è¨­å®šã™ã‚‹", value: "æ‰‹å‹•ã§è¨­å®šã™ã‚‹" }
            ],
            node: "generate_personas",
            key: "personas_error"
          };

          return {
            response,
            nextState: state
          };
        }
        
        const newState: InterviewState = {
          ...state,
          personas,
          iteration: 0,
          is_information_sufficient: false
        };

        // Format personas for display as document
        const personasSummary = personas.map((persona, index) => 
          `## ${index + 1}. ${persona.name}\n\n**èƒŒæ™¯:** ${persona.background}\n`
        ).join('\n');
        
        // Return as completed document
        const response: CompletedDocument = {
          type: "completed",
          documentType: "personas",
          title: "ãƒšãƒ«ã‚½ãƒŠ",
          content: personasSummary,
          node: "generate_personas"
        };

        return {
          response,
          nextState: newState,
          nextNode: "conduct_interviews"
        };
      } catch (error) {
        console.error('Error in generatePersonas:', error);
        // Return error response
        const response: QuestionMessage = {
          type: "question",
          content: "ã‚·ã‚¹ãƒ†ãƒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ãƒšãƒ«ã‚½ãƒŠã®ç”Ÿæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚ã—ã°ã‚‰ããŠå¾…ã¡ã„ãŸã ã„ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚",
          choices: [
            { label: "å†è©¦è¡Œã™ã‚‹", value: "å†è©¦è¡Œã™ã‚‹" }
          ],
          node: "generate_personas",
          key: "system_error"
        };

        return {
          response,
          nextState: state
        };
      }
    }

    // If personas already exist, return as document and auto-progress
    const personasSummary = state.personas.map((persona, index) => 
      `## ${index + 1}. ${persona.name}\n\n**èƒŒæ™¯:** ${persona.background}\n`
    ).join('\n');
    
    const response: CompletedDocument = {
      type: "completed",
      documentType: "personas",
      title: "ãƒšãƒ«ã‚½ãƒŠ",
      content: personasSummary,
      node: "generate_personas"
    };

    return {
      response,
      nextState: state,
      nextNode: "conduct_interviews"
    };
  }

  private async handleConductInterviews(
    state: InterviewState, 
    userResponse?: string
  ): Promise<{
    response: AgentResponse;
    nextState: InterviewState;
    nextNode?: NodeId;
  }> {
    console.log('=== handleConductInterviews ===');
    console.log('User response:', userResponse);
    
    // If user confirmed interviews, move to evaluate information
    if (userResponse && userResponse.includes("ã¯ã„ã€ã“ã®æƒ…å ±ã§è¦ä»¶å®šç¾©ã‚’é€²ã‚ã¦ãã ã•ã„")) {
      console.log('User confirmed interviews, moving to evaluate_information');
      
      const response: ModelPlan = {
        type: "plan",
        nextNode: "evaluate_information"
      };

      return {
        response,
        nextState: state,
        nextNode: "evaluate_information"
      };
    }

    // If interviews already exist, return as document
    if (state.interviews && state.interviews.length > 0) {
      const interviewsSummary = state.interviews.map((interview, index) => 
        `## ${index + 1}. ${interview.persona.name}ã•ã‚“ã¸ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼\n\n**è³ªå•:** ${interview.question}\n\n**å›ç­”:** ${interview.answer}\n`
      ).join('\n');

      const response: CompletedDocument = {
        type: "completed",
        documentType: "interviews",
        title: "ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼çµæœ",
        content: interviewsSummary,
        node: "conduct_interviews"
      };

      return {
        response,
        nextState: state,
        nextNode: "evaluate_information"
      };
    }

    // Conduct interviews if not already done
    const interviews = await this.conductInterviews(state.user_request, state.personas);
    console.log('Conducted interviews:', interviews.length);
    
    const newState: InterviewState = {
      ...state,
      interviews
    };

    // Format interviews for display as document
    const interviewsSummary = interviews.map((interview, index) => 
      `## ${index + 1}. ${interview.persona.name}ã•ã‚“ã¸ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼\n\n**è³ªå•:** ${interview.question}\n\n**å›ç­”:** ${interview.answer}\n`
    ).join('\n');

    const response: CompletedDocument = {
      type: "completed",
      documentType: "interviews",
      title: "ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼çµæœ",
      content: interviewsSummary,
      node: "conduct_interviews"
    };

    return {
      response,
      nextState: newState,
      nextNode: "evaluate_information"
    };
  }

  private async handleEvaluateInformation(state: InterviewState): Promise<{
    response: AgentResponse;
    nextState: InterviewState;
    nextNode?: NodeId;
  }> {
    const evaluation = await this.evaluateInformation(state.user_request, state.interviews);
    
    const newState: InterviewState = {
      ...state,
      is_information_sufficient: evaluation.is_sufficient,
      iteration: state.iteration + 1,
      evaluation_result: evaluation
    };

    let nextNode: NodeId;
    if (evaluation.is_sufficient) {
      nextNode = "generate_professional_requirements";
    } else if (state.followup_round < 2) {
      nextNode = "ask_followups";
    } else {
      nextNode = "generate_professional_requirements"; // è‡ªå‹•è£œå®Œã—ã¦å‰é€²
    }

    const response: ModelPlan = {
      type: "plan",
      nextNode,
      statePatch: { 
        is_information_sufficient: evaluation.is_sufficient,
        iteration: state.iteration + 1,
        evaluation_result: evaluation 
      }
    };

    return {
      response,
      nextState: newState,
      nextNode
    };
  }

  private async handleAskFollowups(
    state: InterviewState, 
    userResponse?: string
  ): Promise<{
    response: AgentResponse;
    nextState: InterviewState;
    nextNode?: NodeId;
  }> {
    console.log('=== handleAskFollowups ===');
    console.log('Has user response:', !!userResponse);
    console.log('Followup round:', state.followup_round);
    console.log('Has evaluation result:', !!state.evaluation_result);

    if (!state.evaluation_result) {
      console.log('No evaluation result, moving to professional requirements');
      const response: ModelPlan = {
        type: "plan",
        nextNode: "generate_professional_requirements"
      };

      return {
        response,
        nextState: state,
        nextNode: "generate_professional_requirements"
      };
    }

    // If we have a user response, append it to the log
    let updatedLog = state.clarification_interview_log;
    if (userResponse) {
      const header = state.followup_round === 0 
        ? "## è¿½åŠ å…¥åŠ›ï¼ˆ1å›ç›®ãƒ»è‡ªç”±è¨˜è¿°ï¼‰" 
        : "## è¿½åŠ å…¥åŠ›ï¼ˆ2å›ç›®ãƒ»ã¯ã„/ã„ã„ãˆï¼‰";
      updatedLog += `\n\n${header}\n${userResponse}`;
    }

    // If this is the second round or no questions, move forward with auto-fill
    if (state.followup_round >= 2 || !state.evaluation_result.followup_questions.length) {
      console.log('Max followup rounds reached or no questions, auto-filling and moving forward');
      
      if (state.evaluation_result.gaps.length > 0) {
        const autoFill = await this.generateAssumptionBackfill(
          state.user_request,
          state.interviews,
          state.evaluation_result.gaps
        );
        updatedLog += "\n\n## è‡ªå‹•è£œå®Œï¼ˆAIä»®è¨­å®šï¼‰\n" + autoFill;
      }

      const newState: InterviewState = {
        ...state,
        clarification_interview_log: updatedLog,
        followup_round: state.followup_round + 1,
        is_information_sufficient: true // Force to move forward
      };

      const response: ModelPlan = {
        type: "plan",
        nextNode: "generate_professional_requirements",
        statePatch: { 
          clarification_interview_log: updatedLog,
          followup_round: newState.followup_round,
          is_information_sufficient: true
        }
      };

      return {
        response,
        nextState: newState,
        nextNode: "generate_professional_requirements"
      };
    }

    const newState: InterviewState = {
      ...state,
      clarification_interview_log: updatedLog,
      followup_round: state.followup_round + 1
    };

    // If we have follow-up questions and no user response yet, ask them
    if (!userResponse && state.evaluation_result.followup_questions.length > 0) {
      console.log('Asking followup questions');
      
      const mode = state.followup_round === 0 ? "è‡ªç”±è¨˜è¿°" : "ã¯ã„/ã„ã„ãˆ";
      const questions = state.followup_round === 0 
        ? state.evaluation_result.followup_questions
        : await this.convertToYesNoQuestions(state.evaluation_result.followup_questions);

      const response: QuestionMessage = {
        type: "question",
        content: `ä»¥ä¸‹ã®ç‚¹ã«ã¤ã„ã¦è¿½åŠ ã§ãŠèã‹ã›ãã ã•ã„ï¼ˆ${mode}å½¢å¼ï¼‰:\n\n${questions.map((q, i) => `${i + 1}. ${q}`).join('\n')}`,
        node: "ask_followups",
        key: "followup_response"
      };

      return {
        response,
        nextState: newState
      };
    }

    // Continue to next step after collecting response
    console.log('Continuing after followup response');
    
    const response: ModelPlan = {
      type: "plan",
      nextNode: "generate_professional_requirements",
      statePatch: { 
        clarification_interview_log: updatedLog,
        followup_round: newState.followup_round
      }
    };

    return {
      response,
      nextState: newState,
      nextNode: "generate_professional_requirements"
    };
  }

  private async handleGenerateProfessionalRequirements(state: InterviewState): Promise<{
    response: AgentResponse;
    nextState: InterviewState;
    nextNode?: NodeId;
  }> {
    console.log('=== handleGenerateProfessionalRequirements ===');
    const requirements = await this.generateProfessionalRequirements(
      state.user_request,
      state.interviews
    );

    console.log('Generated professional requirements:', requirements.length, 'characters');

    const newState: InterviewState = {
      ...state,
      professional_requirements_doc: requirements
    };

    const response: CompletedDocument = {
      type: "completed",
      documentType: "requirements",
      title: "çµ±åˆè¦ä»¶å®šç¾©æ›¸",
      content: requirements,
      node: "generate_professional_requirements"
    };

    console.log('Returning completed requirements document');

    return {
      response,
      nextState: newState,
      nextNode: "analyze_environment"
    };
  }

  private async handleAnalyzeEnvironment(state: InterviewState): Promise<{
    response: AgentResponse;
    nextState: InterviewState;
    nextNode?: NodeId;
  }> {
    console.log('=== handleAnalyzeEnvironment ===');
    const analysis = await this.analyzeExternalEnvironment(state.professional_requirements_doc);

    console.log('Generated environment analysis');

    const newState: InterviewState = {
      ...state,
      consultant_analysis_report: analysis
    };

    const response: CompletedDocument = {
      type: "completed",
      documentType: "analysis",
      title: "å¤–éƒ¨ç’°å¢ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ",
      content: this.formatAnalysisForDisplay(analysis),
      node: "analyze_environment"
    };

    console.log('Returning completed environment analysis document');

    return {
      response,
      nextState: newState,
      nextNode: "assess_profitability"
    };
  }

  private async handleAssessProfitability(state: InterviewState): Promise<{
    response: AgentResponse;
    nextState: InterviewState;
    nextNode?: NodeId;
  }> {
    if (!state.consultant_analysis_report) {
      throw new Error("No analysis report found");
    }

    const assessment = await this.assessProfitability(
      state.professional_requirements_doc,
      state.consultant_analysis_report
    );

    const newState: InterviewState = {
      ...state,
      profitability: assessment
    };

    const response: CompletedDocument = {
      type: "completed",
      documentType: "profitability_assessment",
      title: "åç›Šæ€§è©•ä¾¡",
      content: this.formatProfitabilityForDisplay(assessment),
      node: "assess_profitability"
    };

    return {
      response,
      nextState: newState,
      nextNode: "assess_feasibility"
    };
  }

  private async handleAssessFeasibility(state: InterviewState): Promise<{
    response: AgentResponse;
    nextState: InterviewState;
    nextNode?: NodeId;
  }> {
    if (!state.consultant_analysis_report) {
      throw new Error("No analysis report found");
    }

    const assessment = await this.assessFeasibility(
      state.professional_requirements_doc,
      state.consultant_analysis_report
    );

    const newState: InterviewState = {
      ...state,
      feasibility: assessment
    };

    const response: CompletedDocument = {
      type: "completed",
      documentType: "feasibility_assessment",
      title: "å®Ÿç¾å¯èƒ½æ€§è©•ä¾¡",
      content: this.formatFeasibilityForDisplay(assessment),
      node: "assess_feasibility"
    };

    return {
      response,
      nextState: newState,
      nextNode: "assess_legal"
    };
  }

  private async handleAssessLegal(state: InterviewState): Promise<{
    response: AgentResponse;
    nextState: InterviewState;
    nextNode?: NodeId;
  }> {
    if (!state.consultant_analysis_report) {
      throw new Error("No analysis report found");
    }

    const assessment = await this.assessLegal(
      state.professional_requirements_doc,
      state.consultant_analysis_report
    );

    const newState: InterviewState = {
      ...state,
      legal: assessment
    };

    const response: CompletedDocument = {
      type: "completed",
      documentType: "legal_assessment",
      title: "æ³•çš„ãƒªã‚¹ã‚¯è©•ä¾¡",
      content: this.formatLegalForDisplay(assessment),
      node: "assess_legal"
    };

    return {
      response,
      nextState: newState,
      nextNode: "assessment_gate"
    };
  }

  private async handleAssessmentGate(state: InterviewState): Promise<{
    response: AgentResponse;
    nextState: InterviewState;
    nextNode?: NodeId;
  }> {
    console.log('=== handleAssessmentGate ===');
    console.log('Profitability:', state.profitability);
    console.log('Feasibility:', state.feasibility);
    console.log('Legal:', state.legal);
    
    const profitabilityPassed = state.profitability?.is_profitable ?? false;
    const feasibilityPassed = state.feasibility?.is_feasible ?? false;
    const legalPassed = state.legal?.is_compliant ?? false;
    
    console.log('Assessment results:', {
      profitabilityPassed,
      feasibilityPassed,
      legalPassed
    });
    
    const allPassed = profitabilityPassed && feasibilityPassed && legalPassed;
    console.log('All assessments passed:', allPassed);
    
    const nextNode: NodeId = allPassed ? "generate_pitch" : "improve_requirements";
    console.log('Next node will be:', nextNode);

    const response: ModelPlan = {
      type: "plan",
      nextNode
    };

    return {
      response,
      nextState: state,
      nextNode
    };
  }

  private async handleImproveRequirements(state: InterviewState): Promise<{
    response: AgentResponse;
    nextState: InterviewState;
    nextNode?: NodeId;
  }> {
    console.log('=== handleImproveRequirements ===');
    if (!state.consultant_analysis_report) {
      throw new Error("No analysis report found");
    }

    const badReasons: string[] = [];
    if (state.profitability && !state.profitability.is_profitable) {
      badReasons.push(`[åç›Šæ€§NG] ${state.profitability.reason}`);
    }
    if (state.feasibility && !state.feasibility.is_feasible) {
      badReasons.push(`[å®Ÿç¾æ€§NG] ${state.feasibility.reason}`);
    }
    if (state.legal && !state.legal.is_compliant) {
      badReasons.push(`[æ³•å‹™NG] ${state.legal.reason}`);
    }

    console.log('Bad reasons found:', badReasons.length);
    console.log('Starting requirements improvement...');

    const improvedRequirements = await this.improveRequirements(
      state.professional_requirements_doc,
      state.consultant_analysis_report,
      badReasons
    );

    console.log('Improved requirements length:', improvedRequirements.length);
    console.log('Generating new summary...');

    const newSummary = await this.generateSummaryFromRequirements(improvedRequirements);

    console.log('New summary length:', newSummary.length);

    const newState: InterviewState = {
      ...state,
      professional_requirements_doc: improvedRequirements,
      user_request: newSummary,
      augment_personas: true
      // Keep existing interviews for final pitch generation
    };

    const response: CompletedDocument = {
      type: "completed",
      documentType: "requirements",
      title: "æ”¹å–„ã•ã‚ŒãŸè¦ä»¶å®šç¾©æ›¸",
      content: improvedRequirements,
      node: "improve_requirements"
    };

    console.log('Returning improved requirements document');
    return {
      response,
      nextState: newState,
      nextNode: "generate_pitch" // Skip to final pitch after improvement
    };
  }

  private async handleGeneratePitch(state: InterviewState): Promise<{
    response: AgentResponse;
    nextState: InterviewState;
  }> {
    console.log('=== handleGeneratePitch ===');
    console.log('User request length:', state.user_request.length);
    console.log('Interviews count:', state.interviews.length);
    
    const pitch = await this.generatePitch(state.user_request, state.interviews);
    console.log('Generated pitch length:', pitch.length);
    console.log('Pitch preview:', pitch.substring(0, 200));

    const newState: InterviewState = {
      ...state,
      pitch_document: pitch
    };

    const response: CompletedDocument = {
      type: "completed",
      documentType: "pitch",
      title: "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¼ç”»æ›¸",
      content: pitch,
      node: "generate_pitch"
    };

    console.log('Returning completed pitch document');
    return {
      response,
      nextState: newState
    };
  }

  // ========== LLM Helper Methods ==========
  
  private async generateClarificationQuestions(
    problem: string, 
    persona: string, 
    solution: string
  ): Promise<string> {
    // ã¾ãšAIã®ç†è§£ã‚’2æ–‡ã§è¦ç´„
    const summaryCompletion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ä¸ãˆã‚‰ã‚ŒãŸèª²é¡Œ/ãƒšãƒ«ã‚½ãƒŠ/è§£æ±ºç­–ã‚’2æ–‡ã§æ—¥æœ¬èªè¦ç´„ã€‚å°‚é–€ç”¨èªã¯é¿ã‘ã‚‹ã€‚'
        },
        {
          role: 'user',
          content: `èª²é¡Œ:${problem}\nãƒšãƒ«ã‚½ãƒŠ:${persona}\nè§£æ±ºç­–:${solution}\n2æ–‡ã§è‡ªç„¶ã«è¦ç´„:`
        }
      ],
      temperature: 0.7,
    });

    const summary = summaryCompletion.choices[0]?.message?.content?.trim() || 'è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸã€‚';

    // æœ€åˆã®è³ªå•ã‚’ç”Ÿæˆï¼ˆAIã®ç†è§£ç¢ºèªï¼‰
    return `AIã®ç†è§£ç¢ºèªï¼š
${summary}

ã“ã®ç†è§£ã¯æ­£ã—ã„ã§ã™ã‹ï¼Ÿ`;
  }

  private async generateRequestSummary(
    problem: string,
    persona: string,
    solution: string,
    interviewLog: string
  ): Promise<string> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯å„ªç§€ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ã™ã€‚åˆæœŸå…¥åŠ›ã¨è³ªç–‘å¿œç­”ãƒ­ã‚°ã‚’èª­ã¿è§£ãã€é–‹ç™ºãƒãƒ¼ãƒ ãŒå‚ç…§ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼ã‚’1æ®µè½ã§ç°¡æ½”ã«ä½œæˆã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚'
        },
        {
          role: 'user',
          content: `## å…ƒæƒ…å ±
- **èª²é¡Œ:** ${problem}
- **ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒšãƒ«ã‚½ãƒŠ:** ${persona}
- **è§£æ±ºç­–:** ${solution}

## ãƒ’ã‚¢ãƒªãƒ³ã‚°ãƒ­ã‚°
${interviewLog}

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼:`
        }
      ],
      temperature: 0.7,
    });

    return completion.choices[0]?.message?.content || 'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚';
  }

  private async generatePersonas(userRequest: string): Promise<Persona[]> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ç”¨ã®ãƒšãƒ«ã‚½ãƒŠç”Ÿæˆã®å°‚é–€å®¶ã§ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼ã«åŸºã¥ãã€é©åˆã™ã‚‹å€™è£œãƒšãƒ«ã‚½ãƒŠã‚’5åä½œæˆã—ã¦ãã ã•ã„ã€‚äººç‰©å±æ€§ã®é‡è¤‡ã¯é¿ã‘ã‚‹ã“ã¨ã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã—ã€æ—¥æœ¬åã‚’ç”¨ã„ã‚‹ã“ã¨ã€‚JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã—ã¦ãã ã•ã„ã€‚'
        },
        {
          role: 'user',
          content: `ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼: ${userRequest}

ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§5åã®ãƒšãƒ«ã‚½ãƒŠã‚’è¿”ã—ã¦ãã ã•ã„ï¼š
{
  "personas": [
    {
      "name": "ç”°ä¸­å¤ªéƒ",
      "background": "30ä»£å‰åŠã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã€‚å‰¯æ¥­ã§ã‚¢ãƒ—ãƒªé–‹ç™ºã‚’è¡Œã£ã¦ã„ã‚‹ã€‚"
    }
  ]
}`
        }
      ],
      temperature: 0.8,
    });

    try {
      const content = completion.choices[0]?.message?.content || '{"personas": []}';
      console.log('Raw OpenAI response for personas:', content);
      
      // Try to extract JSON from response if it contains other text
      const jsonMatch = content.match(/\{[\s\S]*\}/);
      const jsonString = jsonMatch ? jsonMatch[0] : content;
      
      const result = JSON.parse(jsonString);
      console.log('Parsed personas result:', result);
      return result.personas || [];
    } catch (error) {
      console.error('Error parsing personas JSON:', error);
      console.error('Raw content was:', completion.choices[0]?.message?.content);
      return [];
    }
  }

  private async conductInterviews(userRequest: string, personas: Persona[]): Promise<Interview[]> {
    const interviews: Interview[] = [];
    
    for (const persona of personas) {
      // Generate 3 questions per persona
      const questions = await this.generateInterviewQuestions(userRequest, persona);
      
      for (const question of questions) {
        const answer = await this.generateInterviewAnswer(persona, question);
        interviews.push({
          persona,
          question,
          answer
        });
      }
    }
    
    return interviews;
  }

  private async generateInterviewQuestions(userRequest: string, persona: Persona): Promise<string[]> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯UXãƒªã‚µãƒ¼ãƒã®è³ªå•è¨­è¨ˆã®å°‚é–€å®¶ã§ã™ã€‚å„ãƒšãƒ«ã‚½ãƒŠã®æ–‡è„ˆã‹ã‚‰ã€çœŸæ„ã‚’å¼•ãå‡ºã™å…·ä½“çš„ãªè³ªå•ã‚’3ã¤ä½œæˆã—ã¦ãã ã•ã„ã€‚å›ç­”ã«æ™‚é–“ãŒã‹ã‹ã‚‰ãªã„ç²’åº¦ã€ã‹ã¤åˆæ„å½¢æˆã«å½¹ç«‹ã¤ã‚‚ã®ã«é™å®šã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚'
        },
        {
          role: 'user',
          content: `ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼: ${userRequest}

å¯¾è±¡ãƒšãƒ«ã‚½ãƒŠ: ${persona.name} - ${persona.background}

ç®‡æ¡æ›¸ã3å•ã§è¿”ã—ã¦ãã ã•ã„ã€‚`
        }
      ],
      temperature: 0.7,
    });

    const response = completion.choices[0]?.message?.content || '';
    return response.split('\n')
      .map(line => line.replace(/^[-â€¢]\s*/, '').trim())
      .filter(line => line.length > 0)
      .slice(0, 3);
  }

  private async generateInterviewAnswer(persona: Persona, question: string): Promise<string> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯ä»¥ä¸‹ã®ãƒšãƒ«ã‚½ãƒŠã¨ã—ã¦å›ç­”ã—ã¾ã™ã€‚ä¸€äººç§°ã§è‡ªç„¶ãªæ—¥æœ¬èªã€2ã€œ3æ–‡ã€å…·ä½“ä¾‹ã‚’äº¤ãˆã‚‹ã“ã¨ã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚'
        },
        {
          role: 'user',
          content: `ãƒšãƒ«ã‚½ãƒŠ: ${persona.name} - ${persona.background}
è³ªå•: ${question}
å›ç­”:`
        }
      ],
      temperature: 0.8,
    });

    return completion.choices[0]?.message?.content || 'å›ç­”ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚';
  }

  private async evaluateInformation(userRequest: string, interviews: Interview[]): Promise<EvaluationResult> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯åŒ…æ‹¬çš„ãªè¦ä»¶æ–‡æ›¸ã‚’ä½œæˆã™ã‚‹ãŸã‚ã®æƒ…å ±ã®ååˆ†æ€§ã‚’è©•ä¾¡ã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚ä¸è¶³ãŒã‚ã‚‹å ´åˆã¯ã€ä½•ãŒè¶³ã‚Šãªã„ã‹ã¨ã€ãã‚Œã‚’åŸ‹ã‚ã‚‹ãŸã‚ã®è¿½åŠ å…¥åŠ›è³ªå•ã‚’å…·ä½“çš„ã‹ã¤å®Ÿè¡Œå¯èƒ½ãªå½¢ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚ãŸã ã—å€‹äººé–‹ç™ºå‰æã«ã¤ãã€è»½å¾®ãªä¸è¶³ã¯AIã®ä»®è¨­å®šã§è£œå®Œå¯èƒ½ã¨åˆ¤æ–­ã—ã€è‡´å‘½çš„ä¸è¶³ã®ã¿ã‚’ä¸ååˆ†ã¨ã™ã‚‹ã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã—ã¦ãã ã•ã„ã€‚'
        },
        {
          role: 'user',
          content: `ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼: ${userRequest}

ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼çµæœ:
${interviews.map(i => `ãƒšãƒ«ã‚½ãƒŠ: ${i.persona.name}\nè³ªå•: ${i.question}\nå›ç­”: ${i.answer}\n`).join('')}

ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è©•ä¾¡çµæœã‚’è¿”ã—ã¦ãã ã•ã„ï¼š
{
  "reason": "åˆ¤æ–­ç†ç”±",
  "is_sufficient": true/false,
  "gaps": ["ä¸è¶³é …ç›®1", "ä¸è¶³é …ç›®2"],
  "followup_questions": ["è¿½åŠ è³ªå•1", "è¿½åŠ è³ªå•2"]
}`
        }
      ],
      temperature: 0.3,
    });

    try {
      const result = JSON.parse(this.cleanJSONFromMarkdown(completion.choices[0]?.message?.content || '{}'));
      return {
        reason: result.reason || '',
        is_sufficient: result.is_sufficient || false,
        gaps: result.gaps || [],
        followup_questions: result.followup_questions || []
      };
    } catch {
      return {
        reason: 'è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸ',
        is_sufficient: false,
        gaps: [],
        followup_questions: []
      };
    }
  }

  private async convertToYesNoQuestions(questions: string[]): Promise<string[]> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯è³ªå•è¨­è¨ˆã®å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸè‡ªç”±è¨˜è¿°ã®ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—è³ªå•ç¾¤ã‚’ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã€Œã¯ã„ï¼ã„ã„ãˆã€ã§ç­”ãˆã‚‰ã‚Œã‚‹å½¢å¼ã«çŸ­æ–‡åŒ–ã—ã¦ãã ã•ã„ã€‚å„è³ªå•ã¯1æ–‡ãƒ»æ—¥æœ¬èªãƒ»è‚¯å®šãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆä»®èª¬ã«ãªã‚‹ã‚ˆã†ã«æ›¸ãæ›ãˆã‚‹ã€‚'
        },
        {
          role: 'user',
          content: `è‡ªç”±è¨˜è¿°ã®è³ªå•ç¾¤:
${questions.map(q => `- ${q}`).join('\n')}

å¤‰æ›å¾Œ: ç®‡æ¡æ›¸ãã§å‡ºåŠ›ã€‚`
        }
      ],
      temperature: 0.3,
    });

    const response = completion.choices[0]?.message?.content || '';
    return response.split('\n')
      .map(line => line.replace(/^[-â€¢]\s*/, '').trim())
      .filter(line => line.length > 0);
  }

  private async generateAssumptionBackfill(
    userRequest: string,
    interviews: Interview[],
    gaps: string[]
  ): Promise<string> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯å€‹äººé–‹ç™ºã®PMã§ã™ã€‚ä»¥ä¸‹ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼/ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼/ä¸è¶³é …ç›®ã«åŸºã¥ãã€ä¸è¶³ã‚’åˆç†çš„ãªä»®è¨­å®šã§è‡ªå‹•è£œå®Œã—ã¾ã™ã€‚å„è£œå®Œã¯ã€Œæ±ºå®šå€¤ï¼ˆ1è¡Œï¼‰ï¼æ ¹æ‹ ï¼ˆ1è¡Œï¼‰ï¼å†ç¢ºèªæ–¹æ³•ï¼ˆ1è¡Œï¼‰ã€ã§çŸ­ãã€‚æ—¥æœ¬èªã§ã€ä¿å®ˆçš„ã‹ã¤å®Ÿè£…å¯èƒ½ãªç¾å®Ÿè§£ã‚’å„ªå…ˆã€‚'
        },
        {
          role: 'user',
          content: `## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼
${userRequest}

## ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ãƒ¡ãƒ¢
${interviews.map(i => `- ${i.persona.name}: ${i.answer}`).join('\n')}

## ä¸è¶³é …ç›®
${gaps.map(g => `- ${g}`).join('\n')}

## å‡ºåŠ›
- é …ç›®å: æ±ºå®šå€¤ / æ ¹æ‹  / å†ç¢ºèªæ–¹æ³•ï¼ˆå„1è¡Œï¼‰ã‚’ç®‡æ¡æ›¸ãã§ã€‚`
        }
      ],
      temperature: 0.5,
    });

    return completion.choices[0]?.message?.content || 'è‡ªå‹•è£œå®Œã«å¤±æ•—ã—ã¾ã—ãŸã€‚';
  }

  private async generateProfessionalRequirements(userRequest: string, interviews: Interview[]): Promise<string> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯ã€å€‹äººé–‹ç™ºè€…ãŒå˜ç‹¬ã§ç€æ‰‹ãƒ»é‹ç”¨ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã®çµ±åˆè¦ä»¶å®šç¾©æ›¸ï¼ˆLeanï¼‹Techï¼‰ã‚’ä½œæˆã™ã‚‹ã€çµŒé¨“è±Šå¯Œãªãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼å…¼ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒŠãƒªã‚¹ãƒˆã§ã™ã€‚ãƒ“ã‚¸ãƒã‚¹å´ï¼ˆLean BRDï¼‰ã¨é–‹ç™ºå´ï¼ˆTech Specï¼‰ã‚’1ã¤ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«çµ±åˆã—ã€ç©ºæ¬„ã‚’ä½œã‚‰ãšä»®èª¬ã§åŸ‹ã‚ã€å®Ÿè¡Œæ‰‹é †ã«è½ã¨ã›ã‚‹ç²’åº¦ã§æ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚'
        },
        {
          role: 'user',
          content: `ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼: ${userRequest}

ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼è©³ç´°:
${interviews.map(i => `ãƒšãƒ«ã‚½ãƒŠ: ${i.persona.name}\nè³ªå•: ${i.question}\nå›ç­”: ${i.answer}\n`).join('')}

ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§çµ±åˆè¦ä»¶å®šç¾©æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

# ğŸ“ çµ±åˆè¦ä»¶å®šç¾©æ›¸ï¼ˆå€‹äººé–‹ç™ºå‘ã‘ï¼šLeanï¼‹Techï¼‰

## A. ãƒ“ã‚¸ãƒã‚¹ï¼ˆLean BRDï¼‰
### A-1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚«ãƒ¼ãƒ‰
### A-2. èª²é¡Œã¨è§£ãç†ç”±ï¼ˆTop3ï¼‰
### A-3. ä¸»è¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¸ãƒ§ãƒ–
### A-4. ä¾¡å€¤ææ¡ˆã¨å·®åˆ¥åŒ–
### A-5. åç›Šãƒ¢ãƒ‡ãƒ«ã¨ä¾¡æ ¼ï¼ˆè©¦ç®—ä»˜ãï¼‰
### A-6. ç²å¾—ãƒãƒ£ãƒãƒ«ã¨æœ€åˆã®10äºº
### A-7. æˆåŠŸæŒ‡æ¨™ï¼ˆNorth Star & KPIï¼‰
### A-8. ã‚¹ã‚³ãƒ¼ãƒ—ã¨å„ªå…ˆé †ä½ï¼ˆMVPå‰æï¼‰
### A-9. ãƒªã‚¹ã‚¯ãƒ»å‰æãƒ»æ³•å‹™
### A-10. ã‚³ã‚¹ãƒˆè¦‹ç©ã¨ãƒ©ãƒ³ãƒ¬ãƒ¼ãƒˆï¼ˆæ¦‚ç®—ï¼‰

## B. é–‹ç™ºï¼ˆTech Specï¼‰
### B-1. MVPãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ï¼ˆ3ã€œ5ä»¶ï¼‰
### B-2. ç”»é¢ã¨ä¸»è¦ãƒ•ãƒ­ãƒ¼
### B-3. ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼ˆç°¡æ˜“ERï¼‰
### B-4. API / å¤–éƒ¨é€£æº
### B-5. éæ©Ÿèƒ½è¦ä»¶ï¼ˆå€‹äººé–‹ç™ºç¾å®Ÿè§£ï¼‰
### B-6. é‹ç”¨ãƒ»ã‚µãƒãƒ¼ãƒˆ
### B-7. é–‹ç™ºãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ï¼ˆ12é€±ç›®å®‰ï¼‰
### B-8. ç”¨èªé›†ï¼ˆæ›–æ˜§èªã®å®šç¾©ï¼‰`
        }
      ],
      temperature: 0.5,
      max_tokens: 4000,
    });

    return completion.choices[0]?.message?.content || 'è¦ä»¶å®šç¾©æ›¸ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚';
  }

  private async analyzeExternalEnvironment(requirements: string): Promise<ExternalEnvironmentAnalysis> {
    try {
      console.log('=== Starting analyzeExternalEnvironment ===');
      console.log('Requirements length:', requirements.length);
      
      const completion = await this.openai.chat.completions.create({
        model: 'gpt-4o',
        messages: [
          {
            role: 'system',
            content: 'ã‚ãªãŸã¯å¤–è³‡ç³»æˆ¦ç•¥ã‚³ãƒ³ã‚µãƒ«ã®ã‚·ãƒ‹ã‚¢ã€‚å€‹äººé–‹ç™ºã®å®Ÿè¡Œå¯å¦åˆ¤æ–­ã«è¶³ã‚‹ç²¾åº¦ã§å¤–éƒ¨ç’°å¢ƒã‚’åˆ†æã™ã‚‹ã€‚3C/PESTã«åŠ ãˆã€JTBDãƒ»å¸‚å ´è¦æ¨¡æ¨å®šãƒ»ãƒãƒ¼ã‚¿ãƒ¼ã®5ãƒ•ã‚©ãƒ¼ã‚¹ãƒ»è¦åˆ¶/è¦ç´„ãƒãƒƒãƒ—ãƒ»GTMãƒ»ãƒ¦ãƒ‹ãƒƒãƒˆã‚¨ã‚³ãƒãƒŸã‚¯ã‚¹ãƒ»æŠ€è¡“å®Ÿç¾æ€§ãƒ»å·®åˆ¥åŒ–/ãƒ¢ãƒ¼ãƒˆãƒ»ä¸»è¦ãƒªã‚¹ã‚¯ï¼†å¯¾ç­–ãƒ»ã‚·ãƒŠãƒªã‚ªã‚’å«ã‚ã€ä¸è¶³æƒ…å ±ã¯æ˜ç¤ºçš„ãªä»®å®šã§è£œå®Œã—ã€æ•°å€¤ã¯ãƒ¬ãƒ³ã‚¸ã¨ç®—å‡ºå¼ã‚’ç¤ºã™ã€‚å‡ºåŠ›ã¯æ—¥æœ¬èªã€Markdownã§ç°¡æ½”ã«ã€‚JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã—ã¦ãã ã•ã„ã€‚'
          },
          {
            role: 'user',
            content: `çµ±åˆè¦ä»¶å®šç¾©æ›¸: ${requirements}

ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å¤–éƒ¨ç’°å¢ƒåˆ†æã‚’è¿”ã—ã¦ãã ã•ã„ï¼š
{
  "customer_analysis": "å¸‚å ´ãƒ»é¡§å®¢åˆ†æã®å†…å®¹",
  "competitor_analysis": "ç«¶åˆåˆ†æã®å†…å®¹", 
  "company_analysis": "è‡ªç¤¾åˆ†æã®å†…å®¹",
  "pest_analysis": "PESTåˆ†æã®å†…å®¹",
  "summary_and_strategy": "è¦ç´„ã¨æˆ¦ç•¥çš„æè¨€ã®å†…å®¹"
}`
          }
        ],
        temperature: 0.3,
        max_tokens: 3000,
      });
      
      console.log('OpenAI API call successful');
      const content = completion.choices[0]?.message?.content;
      console.log('OpenAI response content length:', content?.length || 0);
      console.log('OpenAI response preview:', content?.substring(0, 200) || 'No content');
      
      try {
        // Remove markdown code blocks if present
        const cleanContent = this.cleanJSONFromMarkdown(content || '{}');
        console.log('Cleaned content:', cleanContent.substring(0, 200));
        const result = JSON.parse(cleanContent);
        console.log('JSON parsing successful');
        console.log('Parsed result structure:', {
          customer_analysis_type: typeof result.customer_analysis,
          competitor_analysis_type: typeof result.competitor_analysis,
          company_analysis_type: typeof result.company_analysis,
          pest_analysis_type: typeof result.pest_analysis,
          summary_and_strategy_type: typeof result.summary_and_strategy
        });
        
        // Ensure all fields are strings, converting objects to JSON strings if necessary
        const analysis = {
          customer_analysis: this.ensureString(result.customer_analysis),
          competitor_analysis: this.ensureString(result.competitor_analysis),
          company_analysis: this.ensureString(result.company_analysis),
          pest_analysis: this.ensureString(result.pest_analysis),
          summary_and_strategy: this.ensureString(result.summary_and_strategy)
        };
        console.log('Final analysis object types:', {
          customer_analysis_type: typeof analysis.customer_analysis,
          competitor_analysis_type: typeof analysis.competitor_analysis,
          company_analysis_type: typeof analysis.company_analysis,
          pest_analysis_type: typeof analysis.pest_analysis,
          summary_and_strategy_type: typeof analysis.summary_and_strategy
        });
        console.log('Final analysis object:', analysis);
        
        return analysis;
      } catch (parseError) {
        console.error('JSON parsing failed:', parseError);
        console.error('Raw content:', content);
        return {
          customer_analysis: 'åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆJSONè§£æã‚¨ãƒ©ãƒ¼ï¼‰',
          competitor_analysis: 'åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆJSONè§£æã‚¨ãƒ©ãƒ¼ï¼‰', 
          company_analysis: 'åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆJSONè§£æã‚¨ãƒ©ãƒ¼ï¼‰',
          pest_analysis: 'åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆJSONè§£æã‚¨ãƒ©ãƒ¼ï¼‰',
          summary_and_strategy: 'åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆJSONè§£æã‚¨ãƒ©ãƒ¼ï¼‰'
        };
      }
    } catch (apiError) {
      console.error('OpenAI API call failed:', apiError);
      return {
        customer_analysis: 'åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆAPIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼ï¼‰',
        competitor_analysis: 'åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆAPIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼ï¼‰', 
        company_analysis: 'åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆAPIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼ï¼‰',
        pest_analysis: 'åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆAPIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼ï¼‰',
        summary_and_strategy: 'åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆAPIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼ï¼‰'
      };
    }
  }

  private async assessProfitability(
    requirements: string,
    analysis: ExternalEnvironmentAnalysis
  ): Promise<ProfitabilityAssessment> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯åç›Šæ€§ã®ç›£æŸ»å®˜ã€‚ä¸ãˆã‚‰ã‚ŒãŸè¦ä»¶å®šç¾©æ›¸ã¨å¤–éƒ¨ç’°å¢ƒåˆ†æã‹ã‚‰ã€å€‹äººé–‹ç™ºãŒç¶™ç¶šçš„ã«é»’å­—åŒ–ã§ãã‚‹è¦‹è¾¼ã¿ãŒã‚ã‚‹ã‹ã‚’åˆ¤å®šã™ã‚‹ã€‚ä¾¡æ ¼æˆ¦ç•¥ã€ARPUã€CACã€ç²—åˆ©ã€å›åæœŸé–“ã€ãƒãƒ£ãƒ¼ãƒ³ã€ãƒãƒ£ãƒãƒ«ã®ç¾å®Ÿæ€§ã‚’çŸ­ãåŸå‘³ã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã—ã¦ãã ã•ã„ã€‚'
        },
        {
          role: 'user',
          content: `è¦ä»¶å®šç¾©æ›¸: ${requirements}

å¤–éƒ¨ç’°å¢ƒåˆ†æ:
- é¡§å®¢: ${analysis.customer_analysis}
- ç«¶åˆ: ${analysis.competitor_analysis}
- è‡ªç¤¾: ${analysis.company_analysis}
- PEST: ${analysis.pest_analysis}
- è¦ç´„: ${analysis.summary_and_strategy}

ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§åç›Šæ€§åˆ¤å®šã‚’è¿”ã—ã¦ãã ã•ã„ï¼š
{
  "is_profitable": true/false,
  "reason": "åˆ¤å®šç†ç”±"
}`
        }
      ],
      temperature: 0.3,
    });

    try {
      const result = JSON.parse(this.cleanJSONFromMarkdown(completion.choices[0]?.message?.content || '{}'));
      return {
        is_profitable: result.is_profitable || false,
        reason: result.reason || 'åˆ¤å®šã«å¤±æ•—ã—ã¾ã—ãŸ'
      };
    } catch {
      return {
        is_profitable: false,
        reason: 'åˆ¤å®šã«å¤±æ•—ã—ã¾ã—ãŸ'
      };
    }
  }

  private async assessFeasibility(
    requirements: string,
    analysis: ExternalEnvironmentAnalysis
  ): Promise<FeasibilityAssessment> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯å®Ÿç¾å¯èƒ½æ€§ã®ç›£æŸ»å®˜ã€‚ä¸ãˆã‚‰ã‚ŒãŸè¦ä»¶å®šç¾©æ›¸ã¨å¤–éƒ¨ç’°å¢ƒåˆ†æã‹ã‚‰ã€å€‹äººãŒè² å‚µãªãç¾å®Ÿçš„ãªå·¥æ•°ãƒ»ã‚³ã‚¹ãƒˆãƒ»æŠ€è¡“é›£æ˜“åº¦ã§å®Ÿè£…ãƒ»é‹ç”¨ã§ãã‚‹ã‹ã‚’åˆ¤å®šã™ã‚‹ã€‚MVPã®ç¯„å›²ã€ã‚¹ã‚­ãƒ«å‰æã€æ¨è«–ã‚³ã‚¹ãƒˆ/é…å»¶ã€é‹ç”¨è² è·ã€ä¾å­˜å¤–éƒ¨APIã®åˆ¶ç´„ãªã©ã‚’ç°¡æ½”ã«è©•ä¾¡ã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã—ã¦ãã ã•ã„ã€‚'
        },
        {
          role: 'user',
          content: `è¦ä»¶å®šç¾©æ›¸: ${requirements}

å¤–éƒ¨ç’°å¢ƒåˆ†æ:
- é¡§å®¢: ${analysis.customer_analysis}
- ç«¶åˆ: ${analysis.competitor_analysis}
- è‡ªç¤¾: ${analysis.company_analysis}
- PEST: ${analysis.pest_analysis}
- è¦ç´„: ${analysis.summary_and_strategy}

ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å®Ÿç¾æ€§åˆ¤å®šã‚’è¿”ã—ã¦ãã ã•ã„ï¼š
{
  "is_feasible": true/false,
  "reason": "åˆ¤å®šç†ç”±"
}`
        }
      ],
      temperature: 0.3,
    });

    try {
      const result = JSON.parse(this.cleanJSONFromMarkdown(completion.choices[0]?.message?.content || '{}'));
      return {
        is_feasible: result.is_feasible || false,
        reason: result.reason || 'åˆ¤å®šã«å¤±æ•—ã—ã¾ã—ãŸ'
      };
    } catch {
      return {
        is_feasible: false,
        reason: 'åˆ¤å®šã«å¤±æ•—ã—ã¾ã—ãŸ'
      };
    }
  }

  private async assessLegal(
    requirements: string,
    analysis: ExternalEnvironmentAnalysis
  ): Promise<LegalAssessment> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯æ³•å‹™ãƒ»ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ç›£æŸ»å®˜ã€‚ä¸ãˆã‚‰ã‚ŒãŸè¦ä»¶å®šç¾©æ›¸ã¨å¤–éƒ¨ç’°å¢ƒåˆ†æã‹ã‚‰ã€è‘—ä½œæ¨©ãƒ»å•†æ¨™ãƒ»ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ è¦ç´„ãƒ»å€‹äººæƒ…å ±/ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒ»è¡¨ç¤ºç¾©å‹™ãƒ»å¹´é½¢åˆ¶é™ãªã©ã®è¦³ç‚¹ã§ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãŒé©åˆã—ã¦ã„ã‚‹ã‹ã‚’åˆ¤å®šã™ã‚‹ã€‚é‡å¤§é•åã®æã‚ŒãŒã‚ã‚Œã°Falseã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã—ã¦ãã ã•ã„ã€‚'
        },
        {
          role: 'user',
          content: `è¦ä»¶å®šç¾©æ›¸: ${requirements}

å¤–éƒ¨ç’°å¢ƒåˆ†æ:
- é¡§å®¢: ${analysis.customer_analysis}
- ç«¶åˆ: ${analysis.competitor_analysis}
- è‡ªç¤¾: ${analysis.company_analysis}
- PEST: ${analysis.pest_analysis}
- è¦ç´„: ${analysis.summary_and_strategy}

ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ³•å‹™åˆ¤å®šã‚’è¿”ã—ã¦ãã ã•ã„ï¼š
{
  "is_compliant": true/false,
  "reason": "åˆ¤å®šç†ç”±"
}`
        }
      ],
      temperature: 0.3,
    });

    try {
      const result = JSON.parse(this.cleanJSONFromMarkdown(completion.choices[0]?.message?.content || '{}'));
      return {
        is_compliant: result.is_compliant || false,
        reason: result.reason || 'åˆ¤å®šã«å¤±æ•—ã—ã¾ã—ãŸ'
      };
    } catch {
      return {
        is_compliant: false,
        reason: 'åˆ¤å®šã«å¤±æ•—ã—ã¾ã—ãŸ'
      };
    }
  }

  private async improveRequirements(
    requirements: string,
    analysis: ExternalEnvironmentAnalysis,
    badReasons: string[]
  ): Promise<string> {
    console.log('=== improveRequirements ===');
    console.log('Requirements length:', requirements.length);
    console.log('Bad reasons:', badReasons);
    
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯ã‚·ãƒ‹ã‚¢PMã§ã™ã€‚ä»¥ä¸‹ã®ææ–™ï¼ˆè¦ä»¶å®šç¾©æ›¸ã€å¤–éƒ¨ç’°å¢ƒã€è©•ä¾¡ã®NGç†ç”±ï¼‰ã‚’å—ã‘ã€å€‹äººé–‹ç™ºã§ç¾å®Ÿçš„ã«å‹ã¦ã‚‹å½¢ã¸è¦ä»¶å®šç¾©æ›¸ã‚’æ”¹è¨‚ã—ã¾ã™ã€‚æ”¹è¨‚æ–¹é‡ï¼šMVPã®çµã‚Šè¾¼ã¿ãƒ»å·®åˆ¥åŒ–ã®æ˜ç¢ºåŒ–ãƒ»åç›Šæ€§ã®æ”¹å–„ãƒ»å®Ÿç¾æ€§ã®å‘ä¸Šãƒ»æ³•å‹™ã®é©åˆã®ã„ãšã‚Œã‹ã€‚å…ƒã®è‰¯ã•ã¯ä¿æŒã—ã¤ã¤ã€å±é™ºãªä»®å®šã¯æ˜ç¢ºã«å¤‰æ›´ã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚Markdownã§å®Œçµãªæ”¹è¨‚ç‰ˆã‚’è¿”ã—ã¦ãã ã•ã„ã€‚'
        },
        {
          role: 'user',
          content: `## æ—§ è¦ä»¶å®šç¾©æ›¸
${requirements}

## å¤–éƒ¨ç’°å¢ƒã®è¦ç‚¹
- é¡§å®¢: ${analysis.customer_analysis}
- ç«¶åˆ: ${analysis.competitor_analysis}
- è‡ªç¤¾: ${analysis.company_analysis}
- PEST: ${analysis.pest_analysis}
- è¦ç´„: ${analysis.summary_and_strategy}

## è©•ä¾¡NGç†ç”±
${badReasons.join('\n')}

## å‡ºåŠ›: æ”¹è¨‚ç‰ˆã®è¦ä»¶å®šç¾©æ›¸ï¼ˆMarkdownï¼‰`
        }
      ],
      temperature: 0.5,
      max_tokens: 4000,
    });

    const improvedContent = completion.choices[0]?.message?.content || 'æ”¹è¨‚ã«å¤±æ•—ã—ã¾ã—ãŸã€‚';
    console.log('Improved requirements length:', improvedContent.length);
    return improvedContent;
  }

  private async generateSummaryFromRequirements(requirements: string): Promise<string> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯ç·¨é›†è€…ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸè¦ä»¶å®šç¾©æ›¸ã‹ã‚‰ã€é–‹ç™ºãƒãƒ¼ãƒ å‘ã‘ã«1æ®µè½ã®è¦ç´„ã‚’ä½œæˆã—ã¾ã™ã€‚ãƒˆãƒ¼ãƒ³ã¯ä¸­ç«‹ãƒ»ç°¡æ½”ã€‚å›ºæœ‰åã®ç¾…åˆ—ã‚’é¿ã‘ã€ç›®çš„ãƒ»ä¸»è¦ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ä¾¡å€¤ãƒ»MVPã‚¹ã‚³ãƒ¼ãƒ—ã‚’æ˜ç¤ºã™ã‚‹ã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚'
        },
        {
          role: 'user',
          content: `è¦ä»¶å®šç¾©æ›¸ï¼ˆæŠœç²‹å¯ï¼‰:
${requirements}

---
1æ®µè½ã‚µãƒãƒªãƒ¼:`
        }
      ],
      temperature: 0.3,
    });

    return completion.choices[0]?.message?.content || 'ã‚µãƒãƒªãƒ¼ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚';
  }

  private async generateSummaryFromAnswers(answers: Record<string, string>): Promise<string> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯å„ªç§€ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã§ã™ã€‚åé›†ã•ã‚ŒãŸå›ç­”ã‚’åŸºã«ã€é–‹ç™ºãƒãƒ¼ãƒ ãŒå‚ç…§ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼ã‚’1æ®µè½ã§ç°¡æ½”ã«ä½œæˆã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚'
        },
        {
          role: 'user',
          content: `## åé›†ã•ã‚ŒãŸå›ç­”
${Object.entries(answers).map(([key, value]) => `- ${key}: ${value}`).join('\n')}

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼:`
        }
      ],
      temperature: 0.7,
    });

    return completion.choices[0]?.message?.content || 'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚';
  }

  private formatAnswersAsLog(answers: Record<string, string>): string {
    const sections = [
      { title: "åˆæœŸå•é¡Œ", key: "initial_problem" },
      { title: "AIç†è§£ç¢ºèª", key: "alignment.summary.agree" },
      { title: "ç†è§£ã®å·®åˆ†", key: "alignment.summary.diff" },
      { title: "ä¸»è¦ã‚´ãƒ¼ãƒ«", key: "goal.primary" },
      { title: "æˆåŠŸã®åˆå›³", key: "goal.signal" },
      { title: "ã‚¹ã‚³ãƒ¼ãƒ— In", key: "scope.in" },
      { title: "ã‚¹ã‚³ãƒ¼ãƒ— Out", key: "scope.out" },
      { title: "å„ªå…ˆé †ä½", key: "priority.quality_speed" },
      { title: "å®Œæˆã®å®šç¾©", key: "definition.done" },
      { title: "åˆ¶ç´„ Must", key: "constraints.must" },
      { title: "åˆ¶ç´„ Must-not", key: "constraints.must_not" },
      { title: "å…¥åŠ›", key: "io.input" },
      { title: "å‡ºåŠ›", key: "io.output" },
      { title: "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨æ–‡è„ˆ", key: "user.context" },
      { title: "æ›–æ˜§èª", key: "terms.user_terms" },
      { title: "æ‡¸å¿µãƒ»ãƒªã‚¹ã‚¯", key: "risks.items" }
    ];

    let log = "## ğŸ“‹ è¦ä»¶å®šç¾©ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼çµæœ\n\n";
    
    for (const section of sections) {
      const value = answers[section.key];
      if (value && value.trim()) {
        log += `### ${section.title}\n${value}\n\n`;
      }
    }

    return log;
  }

  private async generateDetailedQuestions(problem: string, persona: string, solution: string): Promise<string[]> {
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯åˆæœŸå…¥åŠ›ï¼ˆèª²é¡Œãƒ»ãƒšãƒ«ã‚½ãƒŠãƒ»è§£æ±ºç­–ï¼‰ã®è§£é‡ˆã¨å¾Œç¶šã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆã®é½Ÿé½¬ã‚’æœ€å°åŒ–ã™ã‚‹ãŸã‚ã®ã€æ–¹å‘æ€§ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆè³ªå•ç¥¨ã€ã‚’ä½œã‚‹å°‚é–€å®¶ã§ã™ã€‚ç‰¹å®šã®æ¥­ç•Œãƒ»åª’ä½“ãƒ»UIãƒ»ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆåã«ä¾å­˜ã—ãªã„æ±ç”¨ã®è³ªå•ã«ã™ã‚‹ã“ã¨ã€‚å…¥åŠ›ï¼ˆèª²é¡Œ/ãƒšãƒ«ã‚½ãƒŠ/è§£æ±ºç­–ï¼‰ã«å«ã¾ã‚Œã‚‹ç”¨èªã‹ã‚‰æ›–æ˜§ã¾ãŸã¯åºƒç¯„ãªèªã‚’æŠ½å‡ºã—ä¸€èˆ¬åŒ–ã—ã¦å®šç¾©ã¥ã‘ã‚’æ±‚ã‚ã‚‹ã€‚å›ç­”ã¯çŸ­æ™‚é–“ã§å¯èƒ½ãªã‚ˆã†é¸æŠä¸­å¿ƒï¼‹æœ€å°é™ã®è‡ªç”±è¨˜å…¥ã€å¿…è¦ãªã‚‰ã€ã‚ã‹ã‚‰ãªã„ã€ã‚’ç”¨æ„ã™ã‚‹ã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚'
        },
        {
          role: 'user',
          content: `ã€å‰æï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã®åˆæœŸå…¥åŠ›ï¼‰ã€‘
- èª²é¡Œ: ${problem}
- ãƒšãƒ«ã‚½ãƒŠ: ${persona}
- è§£æ±ºç­–ã®ä»®èª¬: ${solution}

ä»¥ä¸‹ã®9ã¤ã®è³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚ãã‚Œãã‚Œç°¡æ½”ã§æ˜ç¢ºãªè³ªå•ã«ã—ã€ã€Œã¯ã„/ã„ã„ãˆ/ã‚ã‹ã‚‰ãªã„ã€ã§å›ç­”ã§ãã‚‹ã‚ˆã†ãªå½¢å¼ã«ã—ã¦ãã ã•ã„ï¼š

1. AIã®ç†è§£ç¢ºèªã«é–¢ã™ã‚‹è³ªå•ï¼ˆç†è§£ãŒæ­£ã—ã„ã‹ï¼‰
2. ä¸»è¦ã‚´ãƒ¼ãƒ«ã«é–¢ã™ã‚‹è³ªå•ï¼ˆä¾¡å€¤æ¤œè¨¼/ç²å¾—/åŠ¹ç‡åŒ–/æº€è¶³åº¦/åç›Šã®ã†ã¡ã©ã‚ŒãŒæœ€é‡è¦ã‹ï¼‰
3. ã‚¹ã‚³ãƒ¼ãƒ—Inã«é–¢ã™ã‚‹è³ªå•ï¼ˆä½•ã‚’å«ã‚ã‚‹ã‹ï¼‰
4. ã‚¹ã‚³ãƒ¼ãƒ—Outã«é–¢ã™ã‚‹è³ªå•ï¼ˆä½•ã‚’å«ã‚ãªã„ã‹ï¼‰
5. å„ªå…ˆé †ä½ã«é–¢ã™ã‚‹è³ªå•ï¼ˆå“è³ª vs é€Ÿåº¦ï¼‰
6. å®Œæˆã®å®šç¾©ã«é–¢ã™ã‚‹è³ªå•ï¼ˆã©ã†ãªã‚Œã°å®Œæˆã‹ï¼‰
7. åˆ¶ç´„ã«é–¢ã™ã‚‹è³ªå•ï¼ˆå¿…é ˆæ¡ä»¶ã‚„ç¦æ­¢äº‹é …ï¼‰
8. å…¥å‡ºåŠ›ã«é–¢ã™ã‚‹è³ªå•ï¼ˆä½•ã‚’å…¥åŠ›ã—ã¦ä½•ã‚’å‡ºåŠ›ã™ã‚‹ã‹ï¼‰
9. ãƒªã‚¹ã‚¯ã«é–¢ã™ã‚‹è³ªå•ï¼ˆæ‡¸å¿µç‚¹ã‚„æ³¨æ„ã™ã¹ãç‚¹ï¼‰

å„è³ªå•ã‚’1è¡Œã§ã€ç•ªå·ãªã—ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚`
        }
      ],
      temperature: 0.7,
    });

    const questionsText = completion.choices[0]?.message?.content || '';
    const questions = questionsText.split('\n').filter(q => q.trim().length > 0);
    
    console.log('Generated detailed questions:', questions);
    return questions.slice(0, 9); // æœ€å¤§9ã¤ã«åˆ¶é™
  }

  private formatDetailedAnswersAsLog(questions: string[], answers: Record<string, string>): string {
    let log = "## ğŸ“‹ è©³ç´°è³ªå•ã¨å›ç­”\n\n";
    
    questions.forEach((question, index) => {
      const answer = answers[`detailed_${index}`] || 'æœªå›ç­”';
      log += `### è³ªå• ${index + 1}\n${question}\n**å›ç­”**: ${answer}\n\n`;
    });
    
    return log;
  }

  private async generatePitch(userRequest: string, interviews: Interview[]): Promise<string> {
    console.log('=== generatePitch ===');
    console.log('User request:', userRequest.substring(0, 100));
    console.log('Interviews count:', interviews.length);
    
    const completion = await this.openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯ã€æç¤ºã•ã‚ŒãŸæƒ…å ±ã‚’åŸºã«ã€å¤§å­¦ç”Ÿå‘ã‘ã®é­…åŠ›çš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¼ç”»æ›¸ï¼ˆãƒ”ãƒƒãƒè³‡æ–™ï¼‰ã‚’ä½œæˆã™ã‚‹å­¦ç”Ÿèµ·æ¥­å®¶ã§ã™ã€‚å°‚é–€ç”¨èªã‚’é¿ã‘ã€èª­è€…ãŒå…±æ„Ÿã—ãƒ¯ã‚¯ãƒ¯ã‚¯ã™ã‚‹æ–‡ç« ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿ã§è¨˜è¿°ã™ã‚‹ã“ã¨ã€‚'
        },
        {
          role: 'user',
          content: `ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚µãƒãƒªãƒ¼: ${userRequest}

ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼è©³ç´°:
${interviews.map(i => `ãƒšãƒ«ã‚½ãƒŠã€Œ${i.persona.name}ã€ã®æ„è¦‹: ${i.answer}\n`).join('')}

ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§é­…åŠ›çš„ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¼ç”»æ›¸ã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š

# ğŸš€ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¼ç”»æ›¸: [ã“ã“ã«ã‚­ãƒ£ãƒƒãƒãƒ¼ãªã‚¢ãƒ—ãƒªåã‚’è€ƒæ¡ˆ]

## ğŸ˜µã€Œã“ã‚“ãªã“ã¨ã§å›°ã£ã¦ãªã„ï¼Ÿã€ - è§£æ±ºã—ãŸã„èª²é¡Œ
> [å­¦ç”Ÿå‘ã‘ã®è¨€è‘‰ã§èª²é¡Œã‚’è¡¨ç¾]

## âœ¨ã€Œã“ã†ãªã£ãŸã‚‰æœ€é«˜ã˜ã‚ƒãªã„ï¼Ÿã€ - åƒ•ãŸã¡ã®è§£æ±ºç­–
> [ãƒ™ãƒãƒ•ã‚£ãƒƒãƒˆã‚’æ„Ÿæƒ…çš„ã«æå†™]

## ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼
- **ã“ã‚“ãªäººã«ãƒ”ãƒƒã‚¿ãƒª:** [ä¸€è¡Œã§è¡¨ç¾]

## ğŸ› ï¸ ã“ã®ã‚¢ãƒ—ãƒªã§ã§ãã‚‹ã“ã¨ (ä¸»è¦æ©Ÿèƒ½)
- **[ä¸»è¦æ©Ÿèƒ½1]:** [èª¬æ˜]
- **[ä¸»è¦æ©Ÿèƒ½2]:** [èª¬æ˜] 
- **[ä¸»è¦æ©Ÿèƒ½3]:** [èª¬æ˜]

## ğŸ’° ãƒ“ã‚¸ãƒã‚¹çš„ãªè©±ï¼ˆã¡ã‚‡ã£ã¨ã ã‘ï¼‰
- [ãƒãƒã‚¿ã‚¤ã‚ºã®æ–¹é‡]

## ğŸ¤ ä¸€ç·’ã«ä½œã‚Šã¾ã›ã‚“ã‹ï¼Ÿ
- [å‚åŠ ã‚„å¿œæ´ã®å‘¼ã³ã‹ã‘]`
        }
      ],
      temperature: 0.7,
      max_tokens: 2000,
    });

    const pitchContent = completion.choices[0]?.message?.content || 'ãƒ”ãƒƒãƒç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚';
    console.log('Generated pitch content length:', pitchContent.length);
    return pitchContent;
  }

  private formatAnalysisForDisplay(analysis: ExternalEnvironmentAnalysis): string {
    return `## ğŸ“Š å¤–éƒ¨ç’°å¢ƒåˆ†æãƒ¬ãƒãƒ¼ãƒˆ

### å¸‚å ´ãƒ»é¡§å®¢åˆ†æ
${analysis.customer_analysis}

### ç«¶åˆåˆ†æ
${analysis.competitor_analysis}

### è‡ªç¤¾åˆ†æ
${analysis.company_analysis}

### PESTåˆ†æ
${analysis.pest_analysis}

### è¦ç´„ã¨æˆ¦ç•¥çš„æè¨€
${analysis.summary_and_strategy}`;
  }

  private cleanJSONFromMarkdown(content: string): string {
    // Remove markdown code blocks (```json...``` or ```...```)
    const cleanedContent = content
      .replace(/^```json\s*\n?/i, '')  // Remove opening ```json
      .replace(/^```\s*\n?/i, '')      // Remove opening ```
      .replace(/\n?```\s*$/i, '')      // Remove closing ```
      .trim();
    
    return cleanedContent;
  }

  private ensureString(value: any): string {
    if (typeof value === 'string') {
      return value;
    } else if (value === null || value === undefined) {
      return '';
    } else if (typeof value === 'object') {
      // If it's an object, convert to formatted JSON string
      return JSON.stringify(value, null, 2);
    } else {
      // For numbers, booleans, etc., convert to string
      return String(value);
    }
  }

  private formatProfitabilityForDisplay(assessment: ProfitabilityAssessment): string {
    const status = assessment.is_profitable ? "âœ… åç›ŠåŒ–å¯èƒ½" : "âŒ åç›ŠåŒ–å›°é›£";
    return `## ğŸ’° åç›Šæ€§è©•ä¾¡

### ${status}

${assessment.reason}`;
  }

  private formatFeasibilityForDisplay(assessment: FeasibilityAssessment): string {
    const status = assessment.is_feasible ? "âœ… å®Ÿç¾å¯èƒ½" : "âŒ å®Ÿç¾å›°é›£";
    return `## ğŸ› ï¸ å®Ÿç¾æ€§è©•ä¾¡

### ${status}

${assessment.reason}`;
  }

  private formatLegalForDisplay(assessment: LegalAssessment): string {
    const status = assessment.is_compliant ? "âœ… æ³•çš„å•é¡Œãªã—" : "âš ï¸ æ³•çš„æ³¨æ„ãŒå¿…è¦";
    return `## âš–ï¸ æ³•çš„è©•ä¾¡

### ${status}

${assessment.reason}`;
  }
}